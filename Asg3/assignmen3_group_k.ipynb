{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-WFkdutOHJf"
      },
      "source": [
        "# Assignment 3 - Classification of Image Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW8S4YgnOMpt"
      },
      "source": [
        "## Task 1. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B5eFaBzaYsG"
      },
      "source": [
        "### 1.1 Importing Modules and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPpddL6lQUja",
        "outputId": "4a23d747-3472-481e-8981-450bfac4e164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go3UmhbtQoza",
        "outputId": "7f8d1a94-179e-4915-cd36-ac240e3ab7e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7172 entries, 0 to 7171\n",
            "Columns: 785 entries, label to pixel784\n",
            "dtypes: int64(785)\n",
            "memory usage: 43.0 MB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27455 entries, 0 to 27454\n",
            "Columns: 785 entries, label to pixel784\n",
            "dtypes: int64(785)\n",
            "memory usage: 164.4 MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "test_df = pd.read_csv('/content/gdrive/MyDrive/asg3_comp551/archive/sign_mnist_test.csv')\n",
        "train_df = pd.read_csv('/content/gdrive/MyDrive/asg3_comp551/archive/sign_mnist_train.csv')\n",
        "print(test_df.info())\n",
        "print(train_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3oLPWcbSn2f",
        "outputId": "490e2220-e56a-4588-ff28-0c3bf8b21631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
            "0      3     107     118     127     134     139     143     146     150   \n",
            "1      6     155     157     156     156     156     157     156     158   \n",
            "2      2     187     188     188     187     187     186     187     188   \n",
            "3      2     211     211     212     212     211     210     211     210   \n",
            "4     13     164     167     170     172     176     179     180     184   \n",
            "\n",
            "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
            "0     153  ...       207       207       207       207       206       206   \n",
            "1     158  ...        69       149       128        87        94       163   \n",
            "2     187  ...       202       201       200       199       198       199   \n",
            "3     210  ...       235       234       233       231       230       226   \n",
            "4     185  ...        92       105       105       108       133       163   \n",
            "\n",
            "   pixel781  pixel782  pixel783  pixel784  \n",
            "0       206       204       203       202  \n",
            "1       175       103       135       149  \n",
            "2       198       195       194       195  \n",
            "3       225       222       229       163  \n",
            "4       157       163       164       179  \n",
            "\n",
            "[5 rows x 785 columns]\n",
            "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
            "0      6     149     149     150     150     150     151     151     150   \n",
            "1      5     126     128     131     132     133     134     135     135   \n",
            "2     10      85      88      92      96     105     123     135     143   \n",
            "3      0     203     205     207     206     207     209     210     209   \n",
            "4      3     188     191     193     195     199     201     202     203   \n",
            "\n",
            "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
            "0     151  ...       138       148       127        89        82        96   \n",
            "1     136  ...        47       104       194       183       186       184   \n",
            "2     147  ...        68       166       242       227       230       227   \n",
            "3     210  ...       154       248       247       248       253       236   \n",
            "4     203  ...        26        40        64        48        29        46   \n",
            "\n",
            "   pixel781  pixel782  pixel783  pixel784  \n",
            "0       106       112       120       107  \n",
            "1       184       184       182       180  \n",
            "2       226       225       224       222  \n",
            "3       230       240       253       255  \n",
            "4        49        46        46        53  \n",
            "\n",
            "[5 rows x 785 columns]\n"
          ]
        }
      ],
      "source": [
        "print(train_df.head())\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0xNrfjJajrx"
      },
      "source": [
        "### 1.2 Data Processing - Seperation of Feature and Label with Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5Eg8kyiS3ha"
      },
      "outputs": [],
      "source": [
        "# Seperating the label and features.\n",
        "train_label = train_df['label']\n",
        "test_label = test_df['label']\n",
        "train_feature = train_df.drop(['label'],axis = 1)\n",
        "test_feature = test_df.drop(['label'],axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YadTfrvwWnMp",
        "outputId": "c21988d2-e090-43ab-c6dd-f15324f66213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27455,)\n",
            "(7172,)\n"
          ]
        }
      ],
      "source": [
        "# shape verification of label nparray\n",
        "print(train_label.shape)\n",
        "print(test_label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3Y2lthoUK69",
        "outputId": "ce14c1de-e9bb-427d-888b-85315c2ac01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27455, 784)\n",
            "(7172, 784)\n"
          ]
        }
      ],
      "source": [
        "# vectorizing & shape verification (N X D) s.t. D = 784\n",
        "X_train = train_feature.values\n",
        "X_test = test_feature.values\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tERzIVEpXXpF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb=LabelBinarizer()\n",
        "y_train=lb.fit_transform(train_label)\n",
        "y_test=lb.fit_transform(test_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_BBbuf5ZHpQ",
        "outputId": "3f76b025-cc03-44cb-a04f-af414fe654ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 1 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# check if the one-hot encoding is successful - we compare it with the offered data-processing guidance\n",
        "print(y_train)\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08-JiHL-n7tS",
        "outputId": "0201ed96-f168-45b7-8365-2cb20d21446f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27455, 24)\n",
            "(7172, 24)\n"
          ]
        }
      ],
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxGcBrHOavJO"
      },
      "source": [
        "### 1.3 Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOu5uv98a2BW"
      },
      "source": [
        "Now we perform the normalization on data to prevent scaling bias. This can be done easily by the broatcasting mechanism of numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2RqEkWogJH9"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCDwrCUKc8vK"
      },
      "outputs": [],
      "source": [
        "# centralization\n",
        "central_mean = np.mean(X_train,axis = 0)\n",
        "X_train -= central_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d-onnJpfDAq"
      },
      "source": [
        "For the process of normalization. According to the offered reference (CS231n), we only carry out the min-max normalization once the relative of scales vary much but have equal importance. However, in here, we notice that: 1. pixels are already approximately equal (0 to 255); 2. they are not approximately having euqal importance since the corner pixels usually provides scarce information. Thus, we only carry the std version of normalizaiton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jr-Qu7wepUh"
      },
      "outputs": [],
      "source": [
        "# Normalization - std version\n",
        "normalizing_std = np.std(X_train, axis = 0)\n",
        "X_train /= normalizing_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP7q0yopghlV"
      },
      "source": [
        "We further normalize the testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfoYYUAhglcy"
      },
      "outputs": [],
      "source": [
        "X_test = X_test.astype('float64')\n",
        "X_test -= central_mean\n",
        "X_test /= normalizing_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQLKr2OgyL4"
      },
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC9HytVEg1Eo"
      },
      "source": [
        "From now on, you may use the following:\n",
        "1. `X_train` - training feature matrix\n",
        "2. `X_test` - testing feature matrix\n",
        "3. `Y_train` - trainning label\n",
        "4. `Y_test` - testing label\n",
        "5. `central_mean` - centralization mean\n",
        "6. `normalizing_std` - normalizing std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZBqj2PphKTC"
      },
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNsjlIdOcGjl"
      },
      "source": [
        "### 2.1 MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHGuTtrANKO3"
      },
      "source": [
        "Most classes and functions are borrowed from the NumpyDeepMLP.ipynb. We majorly modified the MLP class to adapt it to a more flexible and concise input."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the parent class of layers used in NN\n",
        "class NeuralNetLayer:\n",
        "    def __init__(self):\n",
        "        self.gradient = None # for gradient calculation\n",
        "        self.parameters = None # storing the corresponding parameters\n",
        "\n",
        "    def forward(self, x): # calculating the parameter and output\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, gradient): # for gradient descent\n",
        "        raise NotImplementedError\n",
        "\n",
        "# parameter layer\n",
        "class LinearLayer(NeuralNetLayer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.ni = input_size # the shape of the W matrix\n",
        "        self.no = output_size\n",
        "        self.w = np.random.randn(output_size, input_size)/np.sqrt(input_size) # standard gaussian initialization to -- weight matrix --\n",
        "        self.b = np.zeros(output_size) # gaus-init --- bias term ---\n",
        "        #self.b = np.random.randn(output_size)\n",
        "        self.cur_input = None # record current input for gradient calculation\n",
        "        self.parameters = [self.w, self.b]\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.cur_input = x # current input vector (update the current property)\n",
        "        return (self.w[None, :, :] @ x[:, :, None]).squeeze() + self.b # shape: batch_size x next_layer_input_size\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        assert self.cur_input is not None, \"Must call forward before backward\"\n",
        "        #dw = gradient.dot(self.cur_input)\n",
        "        dw = gradient[:, :, None] @ self.cur_input[:, None, :]\n",
        "        #dw = np.dot(gradient,self.cur_input)\n",
        "        db = gradient.sum(axis=0)\n",
        "        self.gradient = [dw, db]\n",
        "        return gradient.dot(self.w)\n",
        "\n",
        "# activation layer - hidden\n",
        "class LeakyReLULayer(NeuralNetLayer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.gradient = np.where(x > 0, 1.0, 0.08)\n",
        "        return np.maximum(0.08*x, x)\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        assert self.gradient is not None, \"Must call forward before backward\"\n",
        "        return gradient * self.gradient # backprop - geadient pass to the last layer\n",
        "\n",
        "class ReLULayer(NeuralNetLayer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        assert self.gradient is not None, \"Must call forward before backward\"\n",
        "        return gradient * self.gradient\n",
        "\n",
        "# activation layer - output\n",
        "class SoftmaxOutputLayer(NeuralNetLayer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cur_probs = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        exps = np.exp(x)\n",
        "        probs = exps / np.sum(exps, axis=-1)[:, None]\n",
        "        self.cur_probs = probs\n",
        "        return probs\n",
        "\n",
        "    def backward(self, target):\n",
        "        assert self.cur_probs is not None, \"Must call forward before backward\"\n",
        "        return self.cur_probs - target"
      ],
      "metadata": {
        "id": "VCHR8kEZZhV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define the parent class of layers used in NN\n",
        "class NeuralNetLayer:\n",
        "    def __init__(self):\n",
        "        self.gradient = None # for gradient calculation\n",
        "        self.parameters = None # storing the corresponding parameters\n",
        "\n",
        "    def forward(self, x): # calculating the parameter and output\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, gradient): # for gradient descent\n",
        "        raise NotImplementedError\n",
        "\n",
        "# parameter layer\n",
        "class LinearLayer(NeuralNetLayer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.ni = input_size # the shape of the W matrix\n",
        "        self.no = output_size\n",
        "        self.w = np.random.normal(0,2.0/(input_size + output_size),size = (output_size, input_size)) # standard gaussian initialization to -- weight matrix --\n",
        "        self.b = np.zeros(output_size) # gaus-init --- bias term ---\n",
        "        #self.b = np.random.randn(output_size)\n",
        "        self.cur_input = None # record current input for gradient calculation\n",
        "        self.parameters = [self.w, self.b]\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.cur_input = x # current input vector (update the current property)\n",
        "        return (self.w[None, :, :] @ x[:, :, None]).squeeze() #+ self.b # shape: batch_size x next_layer_input_size\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        assert self.cur_input is not None, \"Must call forward before backward\"\n",
        "        #dw = gradient.dot(self.cur_input)\n",
        "        dw = gradient[:, :, None] @ self.cur_input[:, None, :]\n",
        "        #dw = np.dot(gradient,self.cur_input)\n",
        "        db = gradient.sum(axis=0)\n",
        "        self.gradient = [dw, db]\n",
        "        return gradient.dot(self.w)\n",
        "\n",
        "# activation layer - hidden\n",
        "class LeakyReLULayer(NeuralNetLayer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.gradient = np.where(x > 0, 1.0, 0.08)\n",
        "        return np.maximum(0.08*x, x)\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        assert self.gradient is not None, \"Must call forward before backward\"\n",
        "        return gradient * self.gradient # backprop - geadient pass to the last layer\n",
        "\n",
        "class ReLULayer(NeuralNetLayer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        assert self.gradient is not None, \"Must call forward before backward\"\n",
        "        return gradient * self.gradient\n",
        "\n",
        "# activation layer - output\n",
        "class SoftmaxOutputLayer(NeuralNetLayer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cur_probs = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        exps = np.exp(x)\n",
        "        probs = exps / np.sum(exps, axis=-1)[:, None]\n",
        "        self.cur_probs = probs\n",
        "        return probs\n",
        "\n",
        "    def backward(self, target):\n",
        "        assert self.cur_probs is not None, \"Must call forward before backward\"\n",
        "        print(self.cur_probs.shape)\n",
        "        print(self.cur_probs)\n",
        "        return self.cur_probs - target"
      ],
      "metadata": {
        "id": "ndCcweVP0ilX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cqmCApehpyO"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    # input_size - feature shape (flatten)\n",
        "    # layer_sizes - a list of : # of nodes in the layer\n",
        "    # output_size - output shape (flatten)\n",
        "    def __init__(self, input_size, layer_sizes, output_size, activation_function=ReLULayer,optimizer = None):\n",
        "        self.layers = [] # create the layer list\n",
        "        sizes = [input_size] + layer_sizes + [output_size]\n",
        "        for i in range(len(sizes) - 1): # interpolation of linear layers\n",
        "            self.layers.append(LinearLayer(sizes[i], sizes[i+1]))\n",
        "            if i < len(sizes) - 2:  # interpolation of activation layers\n",
        "                self.layers.append(activation_function())\n",
        "        self.layers.append(SoftmaxOutputLayer())  # multi-classifier activation\n",
        "        self.optimizer = optimizer if optimizer is not None else GradientDescentOptimizer(self, 0.001) # default optimizer\n",
        "        #print(self.layers[len(self.layers)-2].ni,self.layers[len(self.layers)-2].no)\n",
        "        #print(self.layers[len(self.layers)-1].ni,self.layers[len(self.layers)-1].no)\n",
        "\n",
        "    def forward(self, x): # forward automated through the entire hierarchy by input x\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, gradient): # automated backprop start from y\n",
        "        for layer in self.layers[::-1]:\n",
        "            gradient = layer.backward(gradient)\n",
        "            ####print(gradient)\n",
        "        return gradient\n",
        "\n",
        "    def fit(self, X, y, lr, max_iter):\n",
        "        counter = 0\n",
        "        # designation of learning rate (if applicable)\n",
        "        if hasattr(self.optimizer, 'lr'):\n",
        "            self.optimizer.lr = lr\n",
        "        # loss recorder\n",
        "        losses = []\n",
        "        for _ in range(max_iter):\n",
        "            # Compute predictions\n",
        "            predictions = self.forward(X)\n",
        "            # Compute loss (with regularization)\n",
        "            loss = self.compute_loss(X, y)\n",
        "            losses.append(loss)  # Track loss for visualization\n",
        "            # Backpropagation\n",
        "            self.backward(y)\n",
        "            # Update model parameters\n",
        "            self.optimizer.step()\n",
        "            # Print loss (optional)\n",
        "            print(f\"Loss: {loss}\")\n",
        "            counter += 1\n",
        "        return losses\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = self.forward(X)\n",
        "        return np.argmax(predictions, axis=1)\n",
        "\n",
        "    def compute_loss(self, X, y):\n",
        "        preds = self.forward(X)\n",
        "        return -(y * np.log(preds)).sum(axis=-1).mean()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPd39pf7d8nm"
      },
      "source": [
        "    def fit(self, X, y, lr, max_iter):\n",
        "        # designation of learning rate (if applicable)\n",
        "        if hasattr(self.optimizer, 'lr') and self.optimizer.lr != lr:\n",
        "            self.optimizer.lr = lr\n",
        "        for _ in range(max_iter):\n",
        "            preds = self.forward(X)\n",
        "            #gradient = self.backward(y - preds)\n",
        "            gradient = self.backward(y)\n",
        "            self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwEtgy0mHbJZ"
      },
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, net: MLP):\n",
        "        self.net = net # optimizer should attach to the NN\n",
        "\n",
        "    def step(self): # layer-by-layer update\n",
        "        for layer in self.net.layers[::-1]:\n",
        "            if layer.parameters is not None:\n",
        "                self.update(layer.parameters, layer.gradient)\n",
        "\n",
        "    def update(self, params, gradient):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class GradientDescentOptimizer(Optimizer):\n",
        "    def __init__(self, net: MLP, lr: float):\n",
        "        super().__init__(net) # NN initialization\n",
        "        self.lr = lr # learning rate\n",
        "\n",
        "    def update(self, params, gradient):\n",
        "      # layer-by-layer weight update\n",
        "      # p - weight , g - gradient\n",
        "        for (p, g) in zip(params, gradient):\n",
        "            p -= self.lr * g.mean(axis=0) # update based on expected gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNkmEaDsV7N5"
      },
      "source": [
        "class GradientDescentOptimizer(Optimizer):\n",
        "    def __init__(self, net, lr):\n",
        "        self.net = net\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        for layer in self.net.layers:\n",
        "            if hasattr(layer, 'parameters') and layer.parameters is not None:\n",
        "                updated_parameters = []\n",
        "                for param, grad in zip(layer.parameters, layer.gradient):\n",
        "                    param_update = self.lr * grad\n",
        "                    updated_parameters.append(param - param_update)\n",
        "                layer.parameters = updated_parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBgFX0LG_Oj6"
      },
      "source": [
        "class GradientDescentOptimizer(Optimizer):\n",
        "    def __init__(self, net: MLP, lr: float, epsilon=1e-8):\n",
        "        super().__init__(net)\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.acc_sq_grads = {}\n",
        "        \n",
        "        param_index = 0\n",
        "        for layer in self.net.layers:\n",
        "            if hasattr(layer, 'parameters') and layer.parameters is not None:\n",
        "                for param in layer.parameters:\n",
        "                    self.acc_sq_grads[param_index] = np.zeros_like(param)\n",
        "                    param_index += 1\n",
        "\n",
        "    def update(self, params_flat, gradients_flat):\n",
        "        param_index = 0\n",
        "        for layer in self.net.layers:\n",
        "            if hasattr(layer, 'parameters') and layer.parameters is not None:\n",
        "                for i, param in enumerate(layer.parameters):\n",
        "                    # Calculate the accumulated square gradients\n",
        "                    self.acc_sq_grads[param_index] += np.square(gradients_flat[param_index])\n",
        "                    # Compute the updated parameter with AdaGrad adjustment\n",
        "                    adjusted_lr = self.lr / (np.sqrt(self.acc_sq_grads[param_index]) + self.epsilon)\n",
        "                    param -= adjusted_lr * gradients_flat[param_index]\n",
        "                    param_index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAfyukanxI8e"
      },
      "source": [
        "class GradientDescentOptimizer(Optimizer):\n",
        "    def __init__(self, net: MLP, lr: float):\n",
        "        super().__init__(net)\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, gradient):\n",
        "        # Update each parameter based on the gradient\n",
        "        # This modification assumes that 'gradient' is not averaged across the batch\n",
        "        for (p, g) in zip(params, gradient):\n",
        "            # Update the parameter without averaging the gradient across the batch\n",
        "            p -= self.lr * g\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lPsJTJkIY7Q"
      },
      "outputs": [],
      "source": [
        "def evaluate_acc(y_true, y_pred):\n",
        "  print(\"y_true:\",y_true)\n",
        "  print(\"y_pred:\",y_pred)\n",
        "  return np.mean(y_true == y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS78MV-mzh2i"
      },
      "source": [
        "### 2.2 Verfiy Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from scipy.optimize import check_grad\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "def flatten_parameters(mlp):\n",
        "    \"\"\"Flatten all parameters of MLP into a single vector.\"\"\"\n",
        "    params = []\n",
        "    for layer in mlp.layers:\n",
        "        if hasattr(layer, 'parameters') and layer.parameters is not None:\n",
        "            for param in layer.parameters:\n",
        "                params.append(param.flatten())\n",
        "    return np.concatenate(params)\n",
        "\n",
        "def unflatten_parameters(mlp, params):\n",
        "    \"\"\"Unflatten a single vector to set the parameters of MLP.\"\"\"\n",
        "    start = 0\n",
        "    for layer in mlp.layers:\n",
        "        if hasattr(layer, 'parameters') and layer.parameters is not None:\n",
        "            for i, param in enumerate(layer.parameters):\n",
        "                end = start + np.prod(param.shape)\n",
        "                layer.parameters[i] = params[start:end].reshape(param.shape)\n",
        "                start = end\n",
        "\n",
        "def compute_loss_flat(params_flat, mlp, X, y):\n",
        "    \"\"\"Compute loss for a flattened parameter vector.\"\"\"\n",
        "    unflatten_parameters(mlp, params_flat)\n",
        "    preds = mlp.forward(X)\n",
        "    loss = mlp.compute_loss(X, y)\n",
        "    return loss\n",
        "\n",
        "def compute_grad_flat(params_flat, mlp, X, y):\n",
        "    \"\"\"Compute gradient for a flattened parameter vector.\"\"\"\n",
        "    unflatten_parameters(mlp, params_flat)\n",
        "    mlp.forward(X)\n",
        "    mlp.backward(y)\n",
        "    grads_flat = flatten_parameters(mlp)\n",
        "    return grads_flat\n",
        "\n",
        "# Define loss and gradient functions for scipy.optimize.check_grad\n",
        "def loss_func(params_flat):\n",
        "    # Set the parameters to the network\n",
        "    unflatten_parameters(mlp, params_flat)\n",
        "    # Compute loss\n",
        "    return compute_loss_flat(params_flat, mlp, X_subset, y_subset)\n",
        "\n",
        "def grad_func(params_flat):\n",
        "    # Set the parameters to the network\n",
        "    unflatten_parameters(mlp, params_flat)\n",
        "    # Compute and return the gradient\n",
        "    return compute_grad_flat(params_flat, mlp, X_subset, y_subset)\n",
        "\n",
        "# mlp = MLP(input_size=784, layer_sizes=[64, 64], output_size=25)\n",
        "mlp = MLP(input_size=784, layer_sizes=[64, 64], output_size=24)\n",
        "\n",
        "X_subset = X_train[:10]\n",
        "y_subset = y_train[:10]\n",
        "params_flat_initial = flatten_parameters(mlp)\n",
        "\n",
        "# Verify the gradient\n",
        "difference = check_grad(func=loss_func, grad=grad_func, x0=params_flat_initial)\n",
        "\n",
        "print(f\"Gradient check difference: {difference}\")\n"
      ],
      "metadata": {
        "id": "ZgMzYsmbdlqR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3pdMocIzmsN"
      },
      "source": [
        "'''\n",
        "def cross_entropy_loss(predictions, targets):\n",
        "    predictions = np.clip(predictions, 1e-7, 1 - 1e-7) # prevent log(0)\n",
        "    log_preds = np.log(predictions)\n",
        "    loss = -np.sum(log_preds * targets) / predictions.shape[0]\n",
        "    return loss\n",
        "\n",
        "def compute_cost(mlp, X, y):\n",
        "    predictions = mlp.forward(X)\n",
        "    return cross_entropy_loss(predictions, y)\n",
        "\n",
        "def estimate_gradient(mlp, X, y, layer_index, param_type, param_index, epsilon=1e-4):\n",
        "    # get original parameter value\n",
        "    original_param = getattr(mlp.layers[layer_index], param_type)[param_index]\n",
        "\n",
        "    # positive\n",
        "    getattr(mlp.layers[layer_index], param_type)[param_index] = original_param + epsilon\n",
        "    loss_positive = compute_cost(mlp, X, y)\n",
        "\n",
        "    # negative\n",
        "    getattr(mlp.layers[layer_index], param_type)[param_index] = original_param - epsilon\n",
        "    loss_negative = compute_cost(mlp, X, y)\n",
        "\n",
        "    # Reset to original value\n",
        "    getattr(mlp.layers[layer_index], param_type)[param_index] = original_param\n",
        "\n",
        "    # numerical estimate gradient\n",
        "    grad_estimated = (loss_positive - loss_negative) / (2 * epsilon)\n",
        "    return grad_estimated\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxj5XvjdPa6j"
      },
      "source": [
        "'''\n",
        "#init mlp\n",
        "mlp = MLP(input_size=784, layer_sizes=[64, 64], output_size=y_train.shape[1]) # y_train is one-hot encoded\n",
        "\n",
        "# Example usage for a single weight in the first LinearLayer\n",
        "grad_estimated = estimate_gradient(mlp, X_train, y_train, layer_index=0, param_type='w', param_index=(0, 0))\n",
        "print(f\"Estimated gradient: {grad_estimated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7jwIGs7Pp3I"
      },
      "source": [
        "###  2.3 Hyper-parameters turning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "\n",
        "#==================================================================== Sklearn ======================================\n",
        "#rename just to be clear\n",
        "y_train_flat = y_train\n",
        "y_test_flat = y_test\n",
        "\n",
        "# Define hyperparameters\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(32, 32), (64, 64), (128, 128), (256,256)],\n",
        "    'learning_rate_init': [0.001, 0.005, 0.007, 0.01, 0.03],\n",
        "}\n",
        "\n",
        "# Init Classifier\n",
        "mlp = MLPClassifier(max_iter=200)\n",
        "\n",
        "print(\"Initializing GridSearchCV...\")\n",
        "\n",
        "# Setup GridSearchCV\n",
        "# Set n_jobs=-1 to use all available CPUs for parallel computation\n",
        "grid_search = GridSearchCV(mlp, param_grid, scoring=make_scorer(accuracy_score), cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "print(\"Starting GridSearchCV hyperparameter tuning...\")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train_flat)\n",
        "\n",
        "print(\"GridSearchCV tuning completed.\")\n",
        "\n",
        "# Best parameters and best accuracy\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy achieved: {grid_search.best_score_}\")\n",
        "\n",
        "# Predict on the test set with the best found parameters\n",
        "preds = grid_search.predict(X_test)\n",
        "test_acc = accuracy_score(y_test_flat, preds)\n",
        "\n",
        "print(f\"Final test accuracy with best parameters: {test_acc}\")\n"
      ],
      "metadata": {
        "id": "DfOVvAfB5cGV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xez4SIu2QjRh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "7e3c584d-9d8a-4b2c-a7a5-50ebaf3619d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with layer sizes (32, 32) and learning rate 0.001\n",
            "Loss: 3.2772489275992207\n",
            "Loss: 3.227674568812961\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-192cc1944cd5>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Predict on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f6fb5aae5ba3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, lr, max_iter)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Compute loss (with regularization)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Track loss for visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f6fb5aae5ba3>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f6fb5aae5ba3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# forward automated through the entire hierarchy by input x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9228f0ac1a10>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;31m# current input vector (update the current property)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;31m# shape: batch_size x next_layer_input_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "def evaluate_accuracy(predictions, labels):\n",
        "    \"\"\"Compute the accuracy of predictions.\"\"\"\n",
        "    correct_predictions = np.sum(predictions == labels.argmax(axis=1))\n",
        "    accuracy = correct_predictions / labels.shape[0]\n",
        "    return accuracy\n",
        "\n",
        "# Define hyperparameters\n",
        "# hidden_layer_sizes = [(32, 32)]\n",
        "# learning_rate_inits = [0.001, 0.005]\n",
        "hidden_layer_sizes = [(32, 32)]\n",
        "learning_rate_inits = [0.001]\n",
        "best_acc = 0\n",
        "best_params = {}\n",
        "\n",
        "for layer_sizes, lr in product(hidden_layer_sizes, learning_rate_inits):\n",
        "    print(f\"Training with layer sizes {layer_sizes} and learning rate {lr}\")\n",
        "\n",
        "    # Initialize your custom MLP model\n",
        "    mlp = MLP(input_size=784, layer_sizes=list(layer_sizes), output_size=y_train.shape[1])\n",
        "\n",
        "    mlp.fit(X_train, y_train, lr=lr, max_iter=10000)\n",
        "\n",
        "    # Predict on the test set\n",
        "    preds = mlp.predict(X_test)\n",
        "    #test_acc = evaluate_accuracy(preds, y_test)\n",
        "    test_acc = evaluate_accuracy(preds, y_test)\n",
        "    print(f\"Test accuracy: {test_acc}\")\n",
        "\n",
        "    # Update best parameters if the current model is better\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        best_params = {'layer_sizes': layer_sizes, 'learning_rate': lr}\n",
        "\n",
        "print(f\"Best test accuracy: {best_acc} with parameters {best_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9LkgdS1PCSG"
      },
      "source": [
        "## Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ksQt_iJPD_s"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#==================================================================== Sklearn ======================================\n",
        "y_train_flat = np.argmax(y_train, axis=1) if y_train.ndim > 1 else y_train\n",
        "y_test_flat = np.argmax(y_test, axis=1) if y_test.ndim > 1 else y_test\n",
        "\n",
        "hidden_units = [32, 64, 128, 256]\n",
        "models_config = {\n",
        "    \"No hidden layer\": [()],\n",
        "    \"Single hidden layer\": [(h,) for h in hidden_units],\n",
        "    \"Two hidden layers\": [(h, h) for h in hidden_units]\n",
        "}\n",
        "\n",
        "best_scores = {}\n",
        "best_models = {}\n",
        "\n",
        "for model_name, configs in models_config.items():\n",
        "    best_acc = 0\n",
        "    best_config = None\n",
        "    for config in configs:\n",
        "        print(f\"Training {model_name} with hidden units configuration: {config}\")\n",
        "        mlp = MLPClassifier(hidden_layer_sizes=config, max_iter=200, random_state=42)\n",
        "        mlp.fit(X_train, y_train_flat)\n",
        "        preds = mlp.predict(X_test)\n",
        "        acc = accuracy_score(y_test_flat, preds)\n",
        "        print(f\"Accuracy: {acc}\")\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_config = config\n",
        "            best_models[model_name] = mlp\n",
        "\n",
        "    best_scores[model_name] = (best_config, best_acc)\n",
        "    print(f\"Best configuration for {model_name}: {best_config} with accuracy: {best_acc}\\n\")\n",
        "\n",
        "print(\"Summary of best configurations and accuracies:\")\n",
        "for model_name, (config, acc) in best_scores.items():\n",
        "    print(f\"{model_name}: Configuration: {config}, Accuracy: {acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjRojJ1lYWmo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "hidden_units = [32]\n",
        "models_config = {\n",
        "    \"No hidden layer\": [()],\n",
        "    \"Single hidden layer\": [(h,) for h in hidden_units],\n",
        "    \"Two hidden layers\": [(h, h) for h in hidden_units]\n",
        "}\n",
        "\n",
        "best_scores = {}\n",
        "best_models = {}\n",
        "\n",
        "for model_name, configs in models_config.items():\n",
        "    best_acc = 0\n",
        "    best_config = None\n",
        "    for config in configs:\n",
        "        print(f\"Training {model_name} with hidden units configuration: {config}\")\n",
        "        # Adjust the way your MLP is initialized based on its expected parameters\n",
        "        mlp = MLP(input_size=784, layer_sizes=list(config) if config != () else [], output_size=y_train.shape[1], activation_function=LeakyReLULayer)\n",
        "\n",
        "        # Fit your MLP model; ensure your fit method can handle one-hot encoded labels directly\n",
        "        mlp.fit(X_train, y_train, lr=0.001, max_iter=100)\n",
        "\n",
        "        # Predict on the test set\n",
        "        preds = mlp.predict(X_test)\n",
        "\n",
        "        # Convert y_test from one-hot to class labels\n",
        "        y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        acc = accuracy_score(y_test_labels, preds)\n",
        "        print(f\"Accuracy: {acc}\")\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_config = config\n",
        "            best_models[model_name] = mlp\n",
        "\n",
        "    best_scores[model_name] = (best_config, best_acc)\n",
        "    print(f\"Best configuration for {model_name}: {best_config} with accuracy: {best_acc}\\n\")\n",
        "\n",
        "print(\"Summary of best configurations and accuracies:\")\n",
        "for model_name, (config, acc) in best_scores.items():\n",
        "    print(f\"{model_name}: Configuration: {config}, Accuracy: {acc}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}